# -*- coding: utf-8 -*-
"""CLV using RFM analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ag-xFb3dQmp29DB58wBejmRNEl4SmTq

**RFM** is a method used for analyzing customer value and segmenting customers.
This segmentation enables personalized marketing strategies, targeted communication, and, most importantly, helps in predicting future buying behaviors.

Instead of a one-size-fits-all approach, businesses can tailor their marketing efforts to different segments of customers based on their predicted lifetime value.

RFM stands for the three dimensions:

Recency – How recently did the customer purchase?

Frequency – How often do they purchase?

Monetary Value – How much do they spend?
"""

pip install lifetimes

import lifetimes

import pandas as pd
import numpy as np
import datetime as dt

from datetime import timedelta
import plotly.express as px

import matplotlib.pyplot as plt
import seaborn as sns

from lifetimes import BetaGeoFitter
from lifetimes import GammaGammaFitter
from sklearn.preprocessing import MinMaxScaler

from lifetimes.plotting import plot_frequency_recency_matrix
from lifetimes.plotting import plot_probability_alive_matrix
from lifetimes.plotting import plot_period_transactions

df = pd.read_excel('https://github.com/fenago/datasets/raw/main/Online%20Retail.xlsx')

df.head(5)

df.shape

df['TotalSum'] = df['UnitPrice']*df['Quantity']

df.describe()

df.isnull().sum()

import pandas as pd  # Importing the pandas library

# Dropping rows where 'CustomerID' is missing (NaN)
df = df.dropna(subset=['CustomerID'])

# Converting 'CustomerID' first to integer, then to string.
df['CustomerID'] = df['CustomerID'].astype(int).astype(str)

# Converting 'InvoiceDate' to a datetime object.
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

"""# EDA"""

import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Assuming 'df' is your DataFrame with 'UnitPrice' and 'Quantity' columns
fig = make_subplots(rows=1, cols=2)

# Add a vertical boxplot for 'Quantity' in the first column
fig.add_trace(go.Box(y=df['Quantity'], name='Quantity'), row=1, col=1)

# Add a vertical boxplot for 'UnitPrice' in the second column
fig.add_trace(go.Box(y=df['UnitPrice'], name='UnitPrice'), row=1, col=2)

# Update the layout
fig.update_layout(
    title='Boxplots for Quantity and Unit Price',
    height=400,  # Adjust height as needed
    width=800    # Adjust width as needed
)

# Show the plot
fig.show()

"""Negative Quantities?

Removing cancelled products from the dataset
"""

# Removing rows where 'InvoiceNo' contains the letter 'C', 'C' Stands for cancellation
df = df[~df["InvoiceNo"].str.contains("C", na=False)]

# Boxplot visualization after we removed cancellation products
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Assuming 'df' is your DataFrame with 'UnitPrice' and 'Quantity' columns
fig = make_subplots(rows=1, cols=2)

# Add a vertical boxplot for 'Quantity' in the first column
fig.add_trace(go.Box(y=df['Quantity'], name='Quantity'), row=1, col=1)

# Add a vertical boxplot for 'UnitPrice' in the second column
fig.add_trace(go.Box(y=df['UnitPrice'], name='UnitPrice'), row=1, col=2)

# Update the layout
fig.update_layout(
    title='Boxplots for Quantity and Unit Price',
    height=400,  # Adjust height as needed
    width=800    # Adjust width as needed
)

# Show the plot
fig.show()

"""Removing Outliers"""

def remove_outliers(df, column):
    # Calculate IQR and define bounds for outliers
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    # Remove outliers
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
# Apply the function to each numeric column
for column in df.select_dtypes(include='number').columns:
    df = remove_outliers(df, column)

"""# Data Modelling"""

# Setting the snapshot date to the day after the last transaction
snapshot_date = df['InvoiceDate'].max() + timedelta(days=1)
print(snapshot_date)

# Aggregating data per customer
data_process = df.groupby(['CustomerID']).agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days, #Calculate recency as days since last purchase
    'InvoiceNo': 'nunique',  #Count the unique number of invoices as frequency
    'TotalSum': 'sum' #Sum the total monetary value of purchases
})

#Renaming columns to more descriptive names for RFM analysis
data_process.rename(columns={'InvoiceDate': 'Recency',
                         'InvoiceNo': 'Frequency',
                         'TotalSum': 'MonetaryValue'}, inplace=True)

# Rounding the results
data_process = data_process.astype(int)

"""**Distribution of the Recency, Frequency, and Monetary Value Dataset**"""

# Plot RFM distributions
plt.figure(figsize=(12,10))

# Plot distribution of R
plt.subplot(3, 1, 1)
sns.histplot(data_process['Recency'], kde=True)

# Plot distribution of F
plt.subplot(3, 1, 2)
sns.histplot(data_process['Frequency'], kde=True)

# Plot distribution of M
plt.subplot(3, 1, 3)
sns.histplot(data_process['MonetaryValue'], kde=True)

# Show the plot
plt.show()

"""Based on the image, our data distribution is right-skewed, so we need to standardize the data to optimize the model.

# Standardizing Data using StandardScaler
"""

#Import Library
from sklearn.preprocessing import StandardScaler

# Define the feature columns to be standardized.
feature_cols = ['Recency','Frequency','MonetaryValue']

#Create a copy of the dataframe to hold the standardized values. This ensures the original data is not modified.
standardized_data = data_process.copy()

#Initialize the StandardScaler
scaler = StandardScaler()

#Compute the mean and standard deviation for scaling later, using only the feature columns specified earlier.
scaler.fit(data_process[feature_cols])

#Perform standardization by centering and scaling, based on the mean and standard deviation computed above.
standardized_features = scaler.transform(data_process[feature_cols])

#Update the original dataframe with the standardized values for the feature columns.
standardized_data[feature_cols] = standardized_features

# fit K-means clustering on various Ks
from sklearn.cluster import KMeans
kmeans_kwargs = {
    "init" : "random",
    "n_init" : 10
}
sse = []
for k in range(1,11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(standardized_data[feature_cols])
    sse.append(kmeans.inertia_)

import plotly.graph_objects as go

fig = go.Figure()
fig.add_trace(go.Scatter(x=list(range(1, 11)), y=sse, mode='lines+markers'))
fig.update_layout(
    title="Elbow Method for Optimal Number of Clusters",
    xaxis_title="Number of Clusters",
    yaxis_title="Sum of Squared Error"
)
fig.show()

"""k=3"""

from sklearn.cluster import KMeans


# Your KMeans configuration
kmeans_kwargs = {
    "init": "random",
    "n_init": 10,
    "max_iter": 300,  # This can be set to define the maximum number of iterations
    "random_state": 42  # Set the random state for reproducibility
}

# Run KMeans with the optimal k
kmeans = KMeans(n_clusters=3, **kmeans_kwargs)
kmeans.fit(standardized_data[feature_cols])

# Add 1 to each label to change cluster numbering from 1
data_process['cluster'] = kmeans.labels_ + 1

# Reset the index of the DataFrame so 'CustomerID' becomes a regular column
data_process = data_process.reset_index()

# Group by cluster and calculate means and counts
centroid_df = data_process.groupby('cluster').agg({
    'Recency': 'mean',
    'Frequency': 'mean',
    'MonetaryValue': 'mean',
    'CustomerID': 'nunique'
}).reset_index()

# Rename the 'CustomerID' column to 'NumCustomers'
centroid_df = centroid_df.rename(columns={'CustomerID': 'NumCustomers'})

# Sort by 'Frequency' in descending order
centroid_df = centroid_df.sort_values(by='Frequency', ascending=False)

# Add a new column 'MapCluster' based on frequency quartiles
centroid_df['MapCluster'] = pd.qcut(centroid_df['Frequency'], 3, labels=[1, 2, 3])

# Convert 'MapCluster' to integer type
centroid_df['MapCluster'] = centroid_df['MapCluster'].astype(int)

# Merge centroid_df with data_process to add 'MapCluster'
data_process = data_process.merge(centroid_df[['cluster', 'MapCluster']], on='cluster', how='left').reset_index(drop=True)

# Assigning cluster names to the centroids in the dataframe
cluster_names = ['Platinum', 'Gold', 'Silver']
# Adds a new column 'ClusterName' to centroid_df with the provided names
centroid_df['ClusterName'] = cluster_names
# Display the dataframe with the new 'ClusterName' column
centroid_df

